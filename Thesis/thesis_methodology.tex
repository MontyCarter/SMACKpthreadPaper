\chapter{Plan, Timeline, and Scope}\label{thesis_methodology}
The project can be broken up in to two main phases: developing a model for a common subset of the Pthreads API in SMACK, and testing the implementation against benchmarks and refining the model to address any inaccuracies.

I plan to write my thesis concurrently with the completion of my research.   A large portion of my thesis will be dedicated to the details of how certain concurrency constructs were modeled.  As a result, writing the bulk of the thesis while completing research will allow me to get things written up while the details are still fresh.  Though I plan to write my thesis concurrently with research, I have allocated some time after the two main phases for completion of the thesis document.

\section{Model Pthreads API}

\subsection{Identify Subset of API to Model} 

For the first phase,  I will identify the subset of the Pthreads API to be modeled.  This will, of course, include the core set of essential functions such as fork, join, and lock.  However, this will also include all of the most commonly utilized functions found in the benchmarks that will be used to verify the accuracy of the implementation.  

The resulting list of API calls will be prioritized for the most commonly occurring calls.  The number of additional calls, past the set essential functions, that get implemented will depend on the amount of time remaining after implementing the core set of essential functions.  I plan to have the Pthreads API subset selected by February 16th, 2015.

\section{Model Implementation}

Once the API subset has been identified, I will generate a set of regression tests for each API call to determine whether the behavior of the model conforms to the behavior defined in the Pthreads specification.  This is simply a matter of going through the Pthreads specification and generating C programs that test each of the behaviors required of the API calls.  After each API call has quality regressions, I will model the behavior of the call, ensuring that the implemented model passes its regressions.

I expect that this process will be iterative: I will pick one (or a few related) call(s), generate regressions to test for the specified behavior, and implement a model that passes the regressions.  This will be repeated until cutoff date for the Modeling phase of my project, or until the full list of identified Pthread API calls has been modeled.  I intend to set this cutoff date at March 16th, 2015.  This should leave sufficient time for thorough benchmark testing and writing of my thesis.

The very most basic of the Pthreads API calls are already implemented, and are passing a majority of their regressions.  This includes general forking, joining, and basic locking.  However, some of the details of these basic calls are not fully fleshed out.  For example, there is no ``exit()'' primitive in Boogie, so there is currently no way to tell Corral to stop analyzing a thread prematurely.  There are several inaccuracies like this, where the behavior of the model doesn't correctly reflect the semantics of the Pthreads specification.  Such discrepancies need to be corrected.

This core set of essential functions will be the most time consuming to implement, since many of the other API calls involve small variants on a common behavior (such as ``pthread\_mutex\_lock()'' vs ``pthread\_mutex\_trylock()'', where the former blocks the thread until the lock is available, and the latter returns with an error code immediately if the lock is unavailable).  Because of this, I think it is not unreasonable to complete the modeling by the middle of March.

\section{Test Benchmarks}
Once the selected API call models are passing all of their regressions, it will be important to ensure that the regressions were correct, and that accuracy and performance of the implementation is sufficient for real world use.  There are several software verification competitions that include a set of benchmarks for Pthread analysis, as well as several academic projects which we've identified as being a good source for additional test cases.

I will run these benchmark tests against my implementation to assess it for accuracy and performance.  This will likely occur concurrent with the modeling phase of the project for the core set of essential API calls, since any major problems with the model of these core functions would likely be carried over to the variants that use a very similar logic (again, such as lock vs trylock).

After the cutoff date for the Modeling phase, all benchmarks will be tested against the completed models to ensure everything is working as a whole.  I plan to have this benchmarking process completed by March 30, 2015.  I have allotted myself with two weeks for this portion of the project, to provide me with sufficient time to fix any bugs discovered during benchmarking.  

The benchmark testing deadline of March 30th leaves me with about a month to complete my thesis document before the end of the semester.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
